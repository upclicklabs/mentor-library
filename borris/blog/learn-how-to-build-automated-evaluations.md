---
title: "Learn how to build automated evaluations"
source_url: "https://github.com/anthropics/anthropic-cookbook/blob/main/misc/building_evals.ipynb"
source_type: blog
mentor: "Borris"
date_synced: "2026-02-20T00:00:00Z"
---

# Learn how to build automated evaluations

Optimizing Claude to give you the highest possible accuracy on a task is an empirical science, and a process of continuous improvement. Whether you are trying to know if a change to your prompt made the model perform better on a key metric, or whether you are trying to gauge if the model is good enough to launch into production, a good system for offline evaluation is critical to success.

This recipe walks through common patterns in building evaluations, and useful rules of thumb to follow when doing so.

## Parts of an Eval

Evals typically have four parts:

- **An input prompt** that is fed to the model. Often when designing evals the input column will contain a set of variable inputs that get fed into a prompt template at test time.
- **An output** that comes from running the input prompt through the model you want to evaluate.
- **A "golden answer"** to which you compare the model output. The golden answer could be a mandatory exact match, or it could be an example of a perfect answer meant to give a grader a point of comparison.
- **A score**, generated by one of the grading methods discussed below, that represents how the model did on the question.

## Eval Grading Methods

There are three common ways to grade evals:

### 1. Code-based Grading

Using standard code (mostly string matching and regular expressions) to grade the model's outputs. Common versions are checking for an exact match against an answer, or checking that a string contains some key phrase(s). This is by far the best grading method if you can design an eval that allows for it, as it is super fast and highly reliable.

```python
from anthropic import Anthropic

client = Anthropic()
MODEL_NAME = "claude-opus-4-1"
```

```python
def build_input_prompt(animal_statement):
    user_content = f"""You will be provided a statement about an animal and your job is to determine how many legs that animal has.

Here is the animal statement.
<animal_statement>{animal_statement}</animal_statement>

How many legs does the animal have? Return just the number of legs as an integer and nothing else."""
    messages = [{"role": "user", "content": user_content}]
    return messages
```

```python
eval = [
    {"animal_statement": "The animal is a human.", "golden_answer": "2"},
    {"animal_statement": "The animal is a snake.", "golden_answer": "0"},
    {"animal_statement": "The fox lost a leg, but then magically grew back the leg he lost and a mysterious extra leg on top of that.", "golden_answer": "5"},
]
```

```python
def grade_completion(output, golden_answer):
    return output == golden_answer

grades = [grade_completion(output, question["golden_answer"]) for output, question in zip(outputs, eval)]
print(f"Score: {sum(grades) / len(grades) * 100}%")
# Score: 100.0%
```

### 2. Human Grading

A human looks at the model-generated answer, compares it to the golden answer, and assigns a score. This is the most capable grading method as it can be used on almost any task, but it is also incredibly slow and expensive. You should mostly try to avoid designing evals that require human grading if you can help it.

```python
eval = [
    {
        "question": "Please design me a workout for today that features at least 50 reps of pulling leg exercises, at least 50 reps of pulling arm exercises, and ten minutes of core.",
        "golden_answer": "A correct answer should include a workout plan with 50 or more reps of pulling leg exercises (such as deadlifts, but not such as squats which are a pushing exercise), 50 or more reps of pulling arm exercises (such as rows, but not such as presses which are a pushing exercise), and ten minutes of core workouts.",
    },
    {
        "question": "Send Jane an email asking her to meet me in front of the office at 9am to leave for the retreat.",
        "golden_answer": "A correct answer should decline to send the email since the assistant has no capabilities to send emails.",
    },
]
```

### 3. Model-based Grading

Claude is highly capable of grading itself, and can be used to grade a wide variety of tasks that might have historically required humans, such as analysis of tone in creative writing or accuracy in free-form question answering. You do this by writing a grader prompt for Claude.

```python
def build_grader_prompt(answer, rubric):
    user_content = f"""You will be provided an answer that an assistant gave to a question, and a rubric that instructs you on what makes the answer correct or incorrect.

Here is the answer that the assistant gave to the question.
<answer>{answer}</answer>

Here is the rubric on what makes the answer correct or incorrect.
<rubric>{rubric}</rubric>

An answer is correct if it entirely meets the rubric criteria, and is otherwise incorrect.
First, think through whether the answer is correct or incorrect based on the rubric inside <thinking></thinking> tags. Then, output either 'correct' if the answer is correct or 'incorrect' if the answer is incorrect inside <correctness></correctness> tags."""
    messages = [{"role": "user", "content": user_content}]
    return messages
```

```python
import re

def grade_completion(output, golden_answer):
    messages = build_grader_prompt(output, golden_answer)
    completion = get_completion(messages)
    pattern = r"<correctness>(.*?)</correctness>"
    match = re.search(pattern, completion, re.DOTALL)
    if match:
        return match.group(1).strip()
    else:
        raise ValueError("Did not find <correctness></correctness> tags.")
```

## Best Practices

- Make your evals specific to your task whenever possible, and try to have the distribution in your eval represent the real life distribution of questions and question difficulties.
- The only way to know if a model-based grader can do a good job grading your task is to try. Try it out and read some samples to see if your task is a good candidate.
- Often all that lies between you and an automatable eval is clever design. Try to structure questions in a way that the grading can be automated, while still staying true to the task. Reformatting questions into multiple choice is a common tactic.
- In general, your preference should be for higher volume and lower quality of questions over very low volume with high quality.
