---
title: "The Ultimate Guide to AEO Directors Cut - Ethan Smith CEO Graphite"
source_url: "https://radyant.io/masters-of-search/the-ultimate-guide-to-aeo-directors-cut-ethan-smith-ceo-graphite/"
source_type: youtube
mentor: "Ethan"
date_synced: "2026-02-14T00:00:00Z"
---

# The Ultimate Guide to AEO Directors Cut - Ethan Smith CEO Graphite

Graphite is nothing like your typical agency. While most agencies sell slides and strategic decks, Graphite builds things. While others work on 100 item SEO audits, they obsess over the 5% of work that actually drives results. They've proven this approach in SEO with companies like Rippling, Web Flow, Notion, Upwork, and many others. And now they're applying the same ruthless focus

to AI search. That's why I'm excited to have Ethan Smith, CEO of Graphite, on the show today. Ethan has led graphite to the absolute forefront of answer engine optimization as in AEO, one of the largest new marketing channels we've seen in over a decade. He already gave the ultimate guide to AEO on Lenny's podcast a couple of weeks ago, but I felt like a lot of questions remained

unanswered, so I asked him to do a follow-up deep dive and now we're here. So, welcome to the podcast, Ethan. >> Thank you for having me. I could talk about this subject for days. So, let's uh let's answer some questions that haven't been answered yet. >> Okay, let let's let's try let's see if we uh will will end up with a 24-hour recording. Maybe let's start with a

quick recap for everybody that has maybe also not seen um Lenny's um podcast. You and Lenny's podcast. I highly recommend um for everybody to watch or listen to it. Um why should companies care about AEO? I think that they should care about AEO because it's a substantial channel, meaning the amount of impact you can get now is substantial and it's growing.

There's many channels you could focus on. This is one of many. So, I wouldn't say that everyone should be focused on it, but it is one of the channels to consider especially because the the growth curve is is steep. >> And what would you say who should focus on it because you said not everybody should focus on it. >> I think it depends. So, let's talk about

late stage versus early stage. And when uh early stage companies ask me what they should do for SEO, I say do nothing at all or just rank for your brand terms, but don't invest in it because it'll take a long time to drive impact. And if you're a startup, you don't have if you're a seedstage startup, you don't have time to wait 2 years for impact. Whereas, if you're late stage company,

probably most people should be focused on SEO. For answer engine optimization, I think it depends. It probably wouldn't be my number one channel. It might be my fifth most important channel. So if I'm early stage, I would probably do a small amount of stuff, but it wouldn't be my top investment. For early stage and for for SEO, I would I would do nothing for early stage, but for AEO, I would do

something. And the reason why is because you can win answers uh by getting mentioned by other websites. It's unlikely that your URL as a Cstage company will become suddenly part of the citations because uh answer engines have algorithms similar to search algorithms where you need a lot of authority to to rank. And so the time for your website to rank might be a long time, but you

could get mentioned tomorrow by a citation. So for seed stage, it could be interesting because you can get impact quickly. Now I wouldn't focus on thousands and thousands of prompts. I would focus on a small number of them, but uh I probably focused on a few. So I would do a light investment if I was a seed stage or early stage company. And then the larger the company, the more

interesting it is because typically you will have saturated at least the obvious things in search and ads and stuff like that. So it's a new channel that's incremental to what you're already doing. So you probably So like the later the later and larger you are, the more interesting I think it is. You talked also to Lenny about um like who should be responsible for AEO and you you you

gave a good answer already there, but I'd like to dive a little bit deeper into what you think how companies should set up their teams. So if they should rely on their in-house teams like maybe the the marketing team, the SEO team, if they should partner with an agency, what are your thoughts on that? I think there's two different strategies and uh it's offsite and on-site could be the

same person or group. It could be two different ones, but the strate the the skills to do both are are fairly different. For on-site, that's just traditional SEO. So make pages that target prompts, have it the content be good, technical SEO, technical ao, things like that. So that's pretty straightforward. The off-site stuff's the stuff that is somewhat new and the

strategies are actually not that different from SEO, but usually you wouldn't have you usually, at least today, you wouldn't have a team spending a ton of time on backlink building. You might do a little bit of that, but it wouldn't be a core focus of a team. Whereas with answer engines, getting mentioned by citations is actually probably more worth the time than it

would be for link building. So, so essentially, when you say, "ChachiPT, what's the best credit card?" You want the citations to say that you're the best credit card. And if you are uh mentioning the citations, you're suddenly the best credit card. That's not really true where you could suddenly rank, you know, out outrank Chase and Wells Fargo for the best credit card in

Google, but that is true for answer engines. So having an off-site strategy, I think is quite impactful. The kinds of strategies, they vary quite a bit. Uh Reddit, YouTube, LinkedIn, Instagram, G2, these are pretty different from keyword targeting and content optimization, things like that. That doesn't mean that it needs to be a separate team, but frequently those are

two different types of skill sets. So, it may or may not be a separate a separate team. Now, your question was about agency versus in-house. I think it it could be either. I don't think that there's a real distinction. The one thing that I'll mention is that there's not that many people who are masters of Reddit and G2 and LinkedIn and these new channels. So you could either find an

agency who does have experience there or you could find a smart, resourceful uh deals with ambiguity type person which typically you would get in-house more so than an agency. Agencies don't tend to be suddenly inventing brand new strategies, you know, in the middle of a project, whereas somebody inhouse would. So if you have an agency who's already good at these things, great. But if you

don't, I I would I would expect that a uh an in-house person would be more resourceful, more adaptive. >> And how do you usually collaborate at Graphite with companies? So, if we're thinking about like your collaboration with web flow, which is pretty public. So, you've published a lot of things about how you increased um the number of signups from LLMs and also I think Josh

Grant WeP growth at Webflow um um posted a couple of times on LinkedIn about it. Um, how does a good working relationship between an in-house team and an agency look like? Like from your experience at Graphite? >> I think the main thing is when we started with Webflow, it was about SEO and the stuff that we're doing was were not things that we agreed to or and was

not on the so was not in the scope of work and if we stuck stuck to what was in the scope of work, we wouldn't we wouldn't have gotten the impact. So, we need to be able to work with other people, other teams at Web Flow and we need to be able to work on things that we didn't agree to and try out new stuff. And this is part of why I mentioned you you usually would not do

that with an agency. You would usually just have a S. So, so and stick you stick to the so you're not adaptive. We would we try to be adaptive with Web Flow, but we need Web Flow to be open to us working with other teams and you know working on Reddit, working on uh uh YouTube videos, stuff like that that was outside the scope of our original work and that comes from building building

trust with Web Flow and also just building relationships with other parts of Web Flow and for us to be adaptive or for our company. The way that I think about agencies is a it's kind of like a a boat and the bigger the boat, the harder it is to turn. So, if you're in a tiny little boat with one person, you can very quickly turn, but if you're on a cruise liner, you can't turn that

fast. You can do more with a giant cruise liner, but you can't turn quickly. So, we try to be as adaptive as we can. Uh but but uh that's how you can work well with uh someone like us. >> Sounds good. I like the boat analogy. Um, and what would you say, no matter if it's someone looking for hiring people in-house or looking for the right agency or maybe even freelancer to collaborate

with, what would you look for in people that are the right ones to push growth from AEO? There's two things. There's analysis and experiment design and then there's dealing with ambiguity. And I would say that most people in SEO AEO are not great at experiment design and analysis. And that's why you have so many best practices that are not true is because either people never looked at it

or they didn't look at it correctly and they had a false positive where they thought that something had a causal effect and it didn't. So being able to do rigorous analysis and set up an an experiment to figure out what actually works is very important. And we don't know most of what works in answer optimization. We're still early so there aren't a bunch of best practices you can

pull from. You have to do experiments. It's the only way that you know what's working. Otherwise, you're going to waste a bunch of time. Uh, so that's that's skill one. And that's hard to learn on the job. If you show up at a job without knowing how to do experiment design and analysis, you could do that, but I haven't I I don't know if I've ever seen that happen. So, we try to

look for people who already have that skill. And if they don't have that skill, then we assume that they are unlikely to uh develop that skill. Then the second thing is dealing with ambiguity. And I think what makes a good person in answer engine optimization and in growth generally is uh the ability to deal with ambiguous problems. Like if I said why

don't you go figure out a Reddit strategy that's different. That's an ambiguous problem. Whereas I said I want you to take these 10 steps and follow this rubric and uh and and you know replicate what I just showed you. That's a different type of skill. And I would say most people are the second type where they can follow prescriptive process. Uh but they cannot solve an

ambiguous problem. And some problems don't need to be solved like many things have already been solved. A lot of stuff around paid advertising for example there's very established uh processes for exactly what you do. You don't need to reinvent uh how to do bidding. But with figure out Reddit you do need to invent that. And so you need someone who can do experiment design and analysis

and solve ambiguous problems. Let's talk about uh another topic that I found very interesting. You also I think you posted um some some stuff about it already um in your uh in reference to your work with Vimeo um which is video in AO. So I'd like to understand from your point of view why video is important when we talk about AO. I think that UGC and diverse opinions are really important

and video has a lot of that. So if you look at Google, one of the biggest complaints is that the results are all uh derivatives of each other. Everyone is using a content scoring tool and looking at the top 10 results and then rewriting them and all the results are just rewritten versions of each other. And uh and so you have a lack of diversity and then Google uh then wants

to rank Reddit and Twitter and video to increase the amount of diversity. Like if you have one opinion on everything, that's bad. You want many opinions. You want the wisdom of the crowds. The wisdom of the crowd says that if you the more opinions that you have, the the sum of that uh of the group of people will be better than the best expert. So if you said what's how many uh you know how

much does this uh person weigh the if you pulled a thousand people the average often would would be more accurate than the best guess of the best person within the thousand people and so the more diversity the better. So if Google is all rewrites of each other there's no diversity of opinion which which we already have that problem of. So then you want UGC UGC is a great way to have

you know way much larger uh wisdom of the crowd. So then what do LMs want? So LM want the the exact same thing. And LM are especially good at summarization. So the larger the data set, the better the more useful the LM is because it can summarize the opinions of many people. And uh this is related to why answers vary. It's not because it's unpredictable. It's because it's a

probability distribution. And so the bigger the bigger the distribution, the the the better the output. So then coming to video, where do we have the most unique information? We have it in uh text and we have it in videos and images. So in text we have it with Reddit, Kora X, LinkedIn which is why those are huge inputs to LMS and then we have video and video is already a large

input to LMS but the majority of the information in in videos is locked in the video. Most of what's used is the title and the description maybe comments but the majority of the context in the video is not accessible by the LM. I think it will be at some point. So once it and video is already a large data source once you can extract more information from the video. I think that

video will overtake Reddit and overtake X overtake written content because there's so much context in videos. Uh Vimeo who we work with just launched something where you can do a search uh like a semantic search saying I want you to find this type of a scene within a video. So you have you have the the title and the description. That's mostly what's used in the element. Then you

have the transcript which you could make accessible but there's so much uh information that the transcript doesn't really give you like the semantic understanding the one layer of abstraction above the just the words that were said and the the thing that Vimeo just launched they're not like feeding that into an LLM but that type of thing when an LLM can understand

what's happening in a video like for example we're having this conversation uh right now if you just looked at the words that I said there there would there would be a decent amount information, but if you could have a layer of abstraction about the types of things that I'm, you know, the concepts that I'm saying, like I talked about prioritization, I talked about uh I

talked about types of people you could hire. When you add a layer of abstraction above that, then there's so much more unique uh content that could be fed into the LMS. And then you have this massive corpus of unique content and rather than having the 10 rewritten articles that are all the same thing, you have this huge amount of uh uh UGC. So that's why I think the video is so

important today will be even more important, maybe the most important thing over time. Let's quickly tap into um the the the rewritten content problem. So I think this is something that is still happening unfortunately uh a lot. So, and since I know that you guys are also uh creating on-site content, so um not talking about UGC and and the off-site initiatives here, but

on-site content for example for Web Flow or also other companies, how do you ensure that you're not um part of the problem? So, creating like rewritten um content but actually bringing in unique point of view etc. How how do you handle it operationally? >> There's two kinds of content. There's uh there's human content, then there's AI content. Most of what we do is human

content. And uh so part of it diversity and and quality. So quality is easy. Quality you just hire domain experts. Uh it you know it can be expensive, but we have uh we have 13 editors. We have multiple rounds of editing for every single piece of content that we create. We hire uh dedicated domain expert writers for every project that we work on. I think we accept 5% of all uh

applicants. So we're like very rigorous about sourcing the writers and then multiple rounds of editing. So that's on the quality side. And then on the uniqueness side, we try to have uh uniqueness and information gain. So how we could do that would be there's various ways to do that. So you could have expert quotes, you could have unique metadata, you you can have uh

stuff about how to use the product to do the thing like how do I use rippling to pay people in Argentina. So not not just like this is how theoretically to do it but this is how to use the product to do that. So we try to find unique hooks about the company to add some uh amount of uniqueness or information gain. Let's draw from there to go to content workflows a little bit deeper. Um

because obviously we're talking about AEO and we're talking about AI uh as a marketing channel but AI can also be a very powerful tool for a lot of people. And I think from the how people used ChatGpt study that OpenAI did with Harvard for uh professional users actually the biggest category of usage was writing. So I think 40% of people like the the latest data point was 40%

of the usage can be categorized as writing. Um but I know that um you have done I think multiple studies but the latest one was with with was was with with uh Axios um about AI generated content. So my question is um why does AI generated content not work? Yeah. So it does work depending on how you do it. So, usually when people think of I'm going to uh there is this uh a I forget

what it was, the AI heist, the SEO heist where I'm going to take all your content and rewrite it with AI or I'm just going to say chat GPT write a million landing pages, launch all of them. Here are all the keywords, make pages for it, done, free. That doesn't work. And that doesn't work for a few different reasons. Uh I mean, reason one is I think that Google has an AI detector and

they just generally are sensitive to that. Reason two is uh well I mean that's the reason now why shouldn't it work? Well I I'll reframe the question. Why should why should it not work? And so if you just say chatbt write an article about this thing you're der you're essentially deriving that from information that every everyone has access to. So you're adding nothing on

top of uh what what already exists and the similarity will probably be pretty high. And there'll probably be I'm guessing Google has some sort of similarity plus AI footprint. If it's AI but it's unique, it's fine. And if it's AI and it's not unique, it's not fine. That's my guess about how they're algorithmically doing that cuz we have multiple examples where AI content is

working. But it works when you have a unique input. Uh that's not just a you know and it's adding uh uh it's additive. So couple specific examples. Example one is we work with sero which is a a uh community for doctors where they can talk about they gave their patients a particular drug and this is what happened like I gave my patients this particular SSRI

and they had these conditions and this is what happened and this is all proprietary it's closed so it's not in the public and then we have landing pages saying summarize what the doctors are saying are working and not working and that actually works quite well they they rank right underneath WebMD and uh and drugs.com and it's AI generated but it's AI it's an AI generated summary of

unique information really high quality unique information and it's adding you can't find that anywhere else so that works we also have examples with web flow where we have FAQs or product related content and that also works but that's because again it's stuff that's unique to web flow so uh when you're adding like AI is great at summarizing things and if you're summarizing unique

uh information that is not accessible by the public then you're adding and so again algorithmically my guess to what Google's doing is they're saying uh AI detector yes no and then is unique yes no if unique and AI good if uh not unique and AI bad okay makes sense can you give us a little bit more of an understanding of how you use for example um Arabs with web flow because Aerops

from my understanding is also strongly pushing towards AI content generation and like this content engineering movement. So, how do you use it? And what are also your thoughts on the whole content engineering movement? >> I think it's great. I also think AI content is the future. Like clearly it's better if you have AI as a co-pilot than if you don't have it at all. And it's

also bad if you're just rewriting each other's content with no value ad. So, I think clearly you want an AI co-pilot. Just just like with mobile and the internet, there's a bunch of creator apps like there's Instagram and Tik Tok and it's way easier to create videos now than it was in 1980. You would need to spend hundreds of thousands of dollars and now it's free. So, I think that AI

will make it there'll be a whole suite of creator apps. Aerops is more on the professional side. There'll probably be consumer side ones like uh captions and and uh and others but air ops is focused on the uh enterprise professional content workflows. I think it's very interesting. Uh so the kinds of things we've explored there are FAQs. I think also uh things like category pages where

you could have something like an apartments in Santa Monica page and you have unique metadata about the prices and the inventory and you know you you you want a list of apartments and you want to know average price inventory kinds of features. You don't need that doesn't need to be uh you don't need to hire someone from the New York Times to write what the average price of a

apartment in Santa Monica is. Uh but what you do want is you want unique metadata. So then you could use something like an air ops to summarize this metadata. So feed in the unique metadata and get a summarization of that. FAQs. I think that product content is also very interesting. A lot of what people ask in LMS is I want a product with these features and these

attributes. Again, that doesn't need to be someone from the New York Times writing uh you know a beautiful story about whether or not you know you have an integration with uh Rippling. Uh you just need the information. So then an AirOps would be uh good at that as well. So I think that this space is is quite interesting but I you know it shouldn't just be some form on top of chat GBT

where you say write a resume and then it just sort of suddenly gets this thing. You want it to be hooked into workflows such that you have that unique metadata and you're leveraging that as much as possible. >> Got it. We already touched on tooling now with Arabs and I'd like to go a little bit deeper into the whole AEO tool landscape because I know that you

also um published uh a couple of things um about that. So first of all maybe if people are thinking about uh getting an AO tool or I think like marketers always tend to think about hey I need I need a tool for that if there's a new channel. What would you say what should we look for or what should people look for? I think you should look for what do you want to do with it and then what's the

cost and if there's a new feature is it as sold. So what do you need to do with it? You know SEO tools are a good example. There's uh there's screaming frog one of my one of my favorite tools. There's the majority of the columns are not useful. There's a few columns that are useful. So when when I use screaming frog I'll do a site crawl and typically I'm just looking to see the index pages.

[clears throat] How's the internal link coverage? And and so I look at unique inlinks. The rest of the columns are not useful to me. That doesn't mean that they're never useful, but they're not useful to me. So if I had some other Screaming Frog competitor and it was onetenth the cost and it didn't have the metadata, the meta description character count, like I don't care about that. I

don't need that. So what do you actually need? Don't just look for feature list in a series of check marks and like, oh, I need that feature. like what do you actually need? Number one. Number two is uh what's the cost? And I think that the tool space generally is going to rapidly commoditize and we've we're already seeing that. But uh just like SEO tools, there's there's no moat around SEO

tools. So SEO tools are generally a commodity, which is why they everything costs between $80 and $130 a month. Exact same thing will happen with uh AEO tools. I think that an air ops and workflows and stuff like that is different, but I'm just talking about an SCMrush HFS competitor. It's going to cost $80 to $150 uh eventually because the commodity. Uh and then there's new

features. So there's uh there's four features that I think are interesting and probably long-term there's only four features. [clears throat] It's visibility tracking, uh prompt volume, uh workflows, and content scoring. So visibility tracking I think everyone's generally familiar with this concept but for keyword tracking you would use an AHF SEM Rush and say what do I rank and

you rank position three for content workflows let's say and so then for LMS you would say well when I ask what's the best payroll management software I want to know if my brand shows up so this it's how often do you show up and where do you rank I think everyone's familiar with this things to consider with this are the uh where the data come from So

there's the API, there's logged out scraping, and then there's logged in scraping, and then there's paid logged in scraping. These are four different things. They all have uh different types of data. So let's say that I'm calling the API. Uh it's going to give you citations that are very different from logged in citations.

And what you really care about is probably logged in citations like nobody's using the API. And so dep the answers will actually probably not vary that much, but the citations will vary a lot. So if you're doing offsite uh for the seed stage company that I mentioned, if you're using API data, it's going to be way off. You're going to be optimizing citations that are not used

in in logged in. So you want to consider where is the information coming from. And again, for any new feature or data source, I suggest getting multiple sources of truth and comparing them. So rather than just picking one thing and assuming that it's correct, get a few different uh sources of truth. So I would get I would probably use two tools and then I would also compare with

search data. And this brings me to so for visibility tracking, I would manually take questions, log in and manually go into a spreadsheet and put in the citations that you see and compare it. You don't need to do that for thousands of things, but I would at least sanity check how close is this tool to my actual experience. So that's visibility tracking. So then there's

prompt volume. So prompt volume is interesting. This this would be how many people are looking for this prompt. What are the most popular prompts? It's kind of like search volume. How many people are searching for this particular keyword? And so for search, this comes from Google essentially. Uh Google Google Ads API gives you search volume. Google search console gives you

impression counts. Bing gives you impression counts. Bing API gives you search volume. So we have actual firstparty data from four different sources. Google Bing uh search console and Google Bing Ads. And so uh we we we have a lot of good data there. We don't have that for any of the prompts. None of the LMS give us prompt data. So then we have to get data that's as close as

possible to that. [clears throat] So there's uh there's panel data. So what panel data would be would be I have some subset of the total population and that would be coming through uh browser extensions. You bought it from someone like you bought it from a a browser extension. you bought it from an LLM. There's a bunch of LLM apps. I could just pay them and say, "Please give me

your prompt prompt volume." There's uh ISP data. So, there's all these different sources and it's a subset of the total population. So, then the question is how representative is the subset? And the way that I think about this is it's not just about the it's not just about the volume, but it's about how representative it is. So, if I did a presidential poll and I said, "Who are

you going to vote for?" most presidential so the the uh voting population I think is 200 million plus and most presidential polls are a thousand people. So uh and they're I mean you know they're not perfect but they're pretty close. You're not going to be off by 20% for a presidential poll. So it's a thousand people out of 200 million

people which is a tiny fraction of a percent. So the relative sample is extremely low. However, if it's a representative then it can get you pretty close. Now, let's say instead I pull all of California and say, "Who are you going to vote for?" It's going to be much larger sample, but it won't be representative. So, the the the I think it's like 30 million people in

California or something for for voting. So, if I pulled 30 million non-representative versus a thousand representative, it's a way better answer. So, the way to think about panel data is not the size, but the level of representativeness. And so, that's what people are generally using for prompt volume. Now there's discussion about how close particular prompt volumes are and

I come back to you want multiple sources of truth. So I would compare it with search data and how to think about this is that uh search engines get roughly 25 times more page views than LLMs. So therefore you would expect that the prompt volume would be 125th of the search volume generally. It's not exactly the same but uh that's generally what you would expect. So the closer

prompt volume is to search volume, the the more off it is. And so if if you just want to generally sanity check, is this prompt volume reasonable? I would just say the expected relative size is take the search volume, divide by 25. The further away it is from that, the the more inaccurate it is. Let's step into the the visibility um aspect a little bit deeper because we got a uh

question also beforehand from the community and I'm trying to weave in these questions now a little bit. Um if you have a favorite tool to measure visibility tools that I use uh and like we have our own internal tool but we uh we decided to not commercialize that so that's just uh for internal usage and then other tools that we use and like are uh Aerops

has v visibility tracking PKI uh surfer SEO and Everune and I haven't tried the rest so the exclusion of the rest is not because I tried tried them and I didn't like them. It's because I have not tried them. So, I I would say I think I haven't had a bad experience so far. Think that they're all uh good. I I I uh but but again, I I would compare the uh data and the citations with your actual

personal experience. Um what we also do internally is we just have uh actual people logging in and copying and pasting things. Uh so like if if we really care, we're literally just having humans copy and paste things into spreadsheets and uh that's how we do tracking. >> Refreshing. Um another question we got beforehand, it's basically it's a couple

of questions and I feel like they fit in very well at the moment. So the first one is how do you decide which prompts are worth tracking? There's a few different ways to think about this. So there's prompt volume from panels which we talked about. There's search data and then there's uh other conversation data. And um I mostly use the second two. So search data I

think is great because the the the way that that the relative volume of search is probably going to be pretty similar to the relative volume of prompting. The format will probably be different for prompting. Prompts are longer than searches. But I'm you know the the relative number of people looking for payroll management software and search is probably going to be

similar to prompting. And if people related to payroll management software are looking for integrations and or you know uh what's the best ETF with these features, it's probably going to be roughly similar. So for search data, what I do is I either take my existing search data or I take my competitor's search data and pay data. So if I'm a brand new credit card company and I want

to know which prompts to care about, I'm going to go find another credit card company. I'm going to look at what keywords they're bidding on, which are probably going to be the valuable ones, and then transform them into questions. And so, you just just go into hrefs and put in your competitor, get the top paid keywords, export CSV, and then go into chat GPT and say, make a table with

these keywords, and then make a question version of that. That's actually pretty good. [clears throat] It's not going to be exactly what people are prompting, but it's going to be pretty good. It's going to be directionally good. So that's a shortcut for for uh finding uh prompt uh prompt volume and that's what I do. So that's uh option one. Option two is panel data. And I I talked a bit

about this. I don't actually know how accurate it is because I don't know what the panel is. So back to the presidential poll, I don't know anything about the poll. So if I don't know anything about the poll, it might be really accurate, it might not be accurate. I just don't know. So if I don't know, I'm I'll I'll wait until I have some more confidence before I trust

panel data. uh but I know that you know eventually it can work. I just don't know if it works really well today. Then the third would be other conversation data and for prompts the the tail is larger and longer. There's really specific prompts that have never been searched for. So how do we know that what that looks like without with search data? Uh it it necessarily won't have

it. So what how do we get information about the tail? The way to get information about the tail is try to find conversation data in other data sources that you do have access to. So, you have uh conversation uh access to things like your sales calls, your customer support, Reddit, what are people asking about your your brand on Reddit, what are they asking on YouTube,

what are they asking on G2. These are all actual conversations. And this can help us inform what the tail looks like. So, what I would do potentially would be go to chat GBT and say, "Summarize what people on Reddit are asking about my product." Or Reddit actually has Reddit answers where you can say, "Hey, what are people asking about my product?" and then get a summarization of the kinds of

questions people have. So that's how [clears throat] I would fill in the tail. So you know just to summarize search data start with that and that'll tell you most of the information and then Reddit answers G2 tail and then the more we know about how represented the panels are then we can shift to panel data uh whenever that whenever we have confidence around that.

>> And how many prompts do you typically monitor at once or what would you recommend people to monitor? We work with later stage companies and they'll typically do one to 5,000. You know, you you could also ask how many keywords should should you track if if you have no information, how many keywords should you track pro? I mean, ideally, you'd probably be tracking tens

of thousands. Uh if you really want a full full picture, like how many how many keywords should Web Flow track if they want to know how how they're doing? Probably tens of thousands. Uh so how many prompts? probably ideally tens of thousands but I think it's too expensive to actually do that. So I would do at least 1 to 5,000. This is again for enterprise companies which is typically

who we work with. So probably like 1 to 5,000. And then what I would do is I would take all of my top all of my search topics and then make question versions of that. So for web flow there's they want to target product managers and designers and solarreneurs and then there's you know their various features like creative feature persona matrix and then you have your keywords

are you ranking in search and you have your prompts are you ranking for these prompts and you have probably one to 5,000 and then you sort of uh do like 10 prompts per per page and then do this like visibility matrix. >> Okay, great. I think this will surprise a lot of people if they hear a thousand prompts because most start with like 10 to 15. Um, so what would you tell them?

>> Yep. >> I think it's I think it's good to start with 10 10 to 15. And 10 to 15 for a a brand new company or, you know, let's say graphite. Do we need to track 5,000 prompts for ourselves? Probably not. We probably only need to be looking at 10 to 15. Uh, so that's totally fine. It's also a good way to get started. I'm more speaking to if you're a if you're a web

flow and you're a multi-billion dollar company, you probably want something more substantial. So, it just depends on how big your how big your company is and what the footprint of what you want to rank for is. Makes sense. Uh, another question we got and I found this very interesting um is given that LLM outputs can vary significantly because of each user's personalized layer, do you think

tracking prompts is still meaningful and if so, why? I disagree with the premise. So I have not seen a significant difference in answers based on the person. [clears throat] I think that that will happen but I don't think that that's happening now. The majority of the variability of the answers is back to what I described earlier which is that

that the answers are probabilistic. So if you said something like what's the best website builder there's a distribution of potential answers. Web Flow will show up some percent of time and Framer and Lovable and and uh Ghost and like you know they they'll have a probability distribution. Maybe a better example is what's the best flavor of ice cream? Chocolate and vanilla are the

most popular or there's hundreds of kinds of flavors. And so there's a distribution depending on when you ask you might get cinnamon and you might not get cinnamon. Like maybe you get cinnamon 10% of the time but you get chocolate 90% of the time. And so depending on uh you know it's it's like like a coin toss. So depending on when you ask it, you'll get a different

answer. But that's not because it's because I asked it and it's not because the model is changing constantly or it's a black box and unpredictable. It's because it's a probability distribution and you're getting a different answer based on that. I think that someday you will get personalization, but I don't think that's the case today. So then what how do you how do you think about

tracking? You ask the same question multiple times. So the more you ask the question, the more of the view of the distribution you get. We did a study that saw that if you ask about 7 to 10 times, you'll get a decent distribution depending on the answer. For ice cream, you probably need to ask it more times to get the distribution. For website builder, 7 to 10 times is probably fine.

So short answer is I would ask it multiple times and that for most prompts, it'll give you a pretty good sense of the distribution. >> So I just asked Judy, what's the base flavor of ice cream? and it um said the objectively correct answer is vanilla. And I asked it the second time and it also said vanilla. So uh my anecdotal evidence says vanilla is objectively the

best flavor of ice cream. Do you agree? >> Did it did it mention any other flavors? >> Yeah. Yeah. It mentioned um that uh chocolate uh so depending on your mood chocolate and then I think there it got a little bit fancy and it also says salted caramel when you want to feel a bit fancy without committing to chaos. >> Salted caramel. And if you asked it again right now without cashing, I will

bet you it will not say salted caramel. I bet you it'll say some other uh type of ice cream. >> Yeah. I got cookies and cream, pistachio, and then mint chocolate chip. Yeah. So, I got chocolate, mint, chocolate chip, pistachio, cookies, and cream. And yeah, that's what I got. So, so mostly overlap, >> but not salted caramel.

>> Yeah, salted caramel was the fancy one if you feel a bit fancy. >> Yeah, maybe that shows up 5% of the time. >> So, now this transition, I'm very proud of the transition that will come. Now, we talked about ice cream. Now let's talk about pies [laughter] because I want to talk about incrementality between SEO and AEO. Um

so I saw your case study um from web flow where you basically doubled um the percentage of um signups from LLMs um which is obviously something that a lot of people are very uh interested in. So because it's uh I think one of the first like real publicly documented um cases of um people getting a lot of business value from LLMs. So but my question is isn't the gain you get from LLMs the

pain so to say you lose from Google? So is the pie really getting bigger or is it just a shift from one to the other? The pie is getting bigger. So based on my data and my data, so there's a few ways to look at this. Ultimately, I think what you would want to look at is incremental conversions across tens of thousands of companies like across every company in the United States or in the

world. Uh are the total number of conversions increasing? I don't actually know the answer to that. I don't have the data for that. I do have data for individual companies. One step before that would be is the usage incremental? So that I do know. If you look at similar web data, you'll see that visits for LLMs or page views are 125th the size of search and they're generally

incremental. Uh Google is Google's not going down and maybe it's going down a tiny tiny amount but if you sum them it's incremental. The pi is getting larger. Okay. And what about professional users? Because if we think about uh maybe you and me also, I can see us working in our day-to-day a lot with AI tools and I think the the probability of us then

also turning to claw chubity perplexity for research is higher. But isn't it plausible that one prompt or maybe two prompts we do with CHBT is a substitute to let's say six seven searches we would have done on Google earlier. Definitely. So my statement is about the macro effect. You're asking about the micro effect. What about different specific use cases? I definitely think

that uh LMS are used more for specific use cases. LMS are very good at things like analysis, summarization of large data sets, research, and I would expect that people would use them instead of search because they're much better at that than searches, just like I would expect for video. Uh people would use people would look for travel ideas in Instagram and Tik Tok and YouTube more

than they might look in Google. They might look in Reddit more than they might look in Google. Same with beauty and things like that. Like there's different services are better uh better at fulfilling the use case. Same with LMS. But I but I don't think that search is generally going down. I think that these new surfaces like YouTube, Instagram, Tik Tok, Reddit, uh LMS are

additive to search. uh the pie is getting larger, the slice for search is the same size and you're just adding slices on top. And would you say that AEO as a marketing channel is um like has the same potential across different verticals? So if we think about beauty, if we think about um tech products, um if we think about B2B SAS, if we think about um the your example the the doctor

platform, so so so medical topics, would you say there are differences in how important or how impactful AO can be across these verticals? Definitely. I mean the relative size of these is is where the impact is. You mentioned a few specific examples also in looking at some Reddit data. It looks like the kinds of things people are discussing on Reddit are uh it uh LLMs

are especially useful for things like analysis uh coding. Interestingly, mental health is is another area where people are using LMS. I'm trying to dig into that more. Uh help with education and learning, help with research, ideas, brainstorming. I think that LMS are very good. We talked about a sea of sameness where LLMs are derivatives of existing information. It's actually great at

suggesting ideas for information gain. Like if you said, "What are some non-obvious questions about website builders?" It would actually give you great or like what kinds of uh what kinds of people aren't using website no code website builders and they should be. It'll give you really good ideas for that. So, uh it's actually great at brainstorming, coming up with new ideas.

Has your own research behavior shifted like from Google to CHPT cla or some other tool? I use LLMs for research for summarization. Actually, for some of my research, it's great for trying to see if they're academic journal articles about a thing that I'm exploring. It was quite painful to look for. There's millions of probably millions of

academic journal articles. There's no way I can search through all these. But it's actually great at saying, "Has there been any prior research on uh AI content?" Not, not really. Okay, cool. Well, you know, if there are, here's what they said. I'm uh I'm looking at uh uh evaluating the effectiveness of AI detectors. And there's actually very few studies on this and the sample sizes are

really low. But I created a table of all the different uh studies about uh about the evaluation of the effectiveness of AI detectors and the sample sizes and where the samples came from and that would have I would have you know taken hours to do that in Google search and now I can do that in a few seconds in LM. So so these are the some of some of the things that I use it for. I'll also

say I went to New York and there are all these different things to do in New York. There were these different shows. And so then I asked Reddit answers, "Is this event good?" And I went to Masquerade, which is this Phantom of the Opera immersive theater, and the Reddit answers said, "Yes, this is a great event." And then there was another one where it was like a light show, and I

said, "What do you think? What does Reddit think about this?" And they said, "Well, people say it's overpriced and it's kind of lame." So whereas in Google, I wouldn't get that. So, uh, Reddit is a great source of information for things to do. Similar, I went to Hawaii and then I said, "Well, what do people say you should do in Kawaii?" Everyone says do the helicopter and make

sure you don't have the doors on. So I did that. Had a great time. Uh but but yeah, these are the some some of the ways that I uh use LMS. >> Okay. Awesome. Um we touched already a little bit on um the results that you have gotten already from AEO, especially for example from Webflow. Um, so I'd like to focus on the topic of attribution a little bit because I feel

like people are still having a hard time seeing um, showing up in AI answers being connected to driving real business results in terms of signups in terms of pipeline um, um, contribution. So from your perspective and your experience, what's the issue with attribution in LLMs? The attribution issue is that most of the answers are are no click. So if you

say what's the best payroll management software, you probably aren't going to click on Rippling, you're probably going to open a new tab and you'll either search for Ripling in Google and then click on their homepage or their ad their ad where they're bidding they have to bid on their own brand term [clears throat] or they'll open a new tab and type in rippling.com.

So either you're attributing it to direct branded search or branded paid search and but it actually came because the answer said rippling's great and uh and this is really hard to track. Now sometimes there's something to click on but most of the time there's not. So the majority of cases where you actually got a conversion there's not something that's traceable and clickable. So

that's that's the main issue. The second issue uh is is uh similar to SEO where there's a user journey for for Ripling. you probably heard of them a hundred times. So there's so many touch points. So if you just looked at the last thing that they did before they converted, you're missing the previous 99 things that happened. And um and and even though you can trace the click, you

still don't know what happened. Uh you have a very skewed view of what happened right before that. And so that's why you need to do things like multi-touch or mixed media modeling or uh just ask them how did you hear about us? That's search. Now, answer management optimization gets even more messy because you don't actually know the volume of the prompt. So, like you you

could track I'm showing up for this prompt. You don't know how many people are looking for that. Uh you have to ask multiple times to see where you appear. There's no click-through rate. So, who knows if somebody if you know if if you were at the top or the bottom like did somebody see you? We don't know. So, there's all these compounding uh pieces of compounding errors on top

of the fact that it's not traceable. So that's why it's hard. So my suggestion is focus on the beginning and the end. The beginning would be did I appear for these prompts? Yes. No. And was I was I ranked high? And just make a guess about what the volume might be. Probably based on search data. Uh and then for conversions, ask the person how where did you come from? And uh and

they'll tell you. Now there's issues with self-reporting. uh clearly who knows what the weight of that you know they said it was from this one source but we know that it was from many different sources what were the weight of all those sources we don't know but uh that's the closest you can get I think is did I appear for prompts with assumed volume with large error rate and

then what did people tell me after they converted >> very messy [laughter] >> yeah already sounded a little messy um how did you how did you solve um the attribution ution at Webflow or what what you can maybe share about um the case with web flow where you were able to actually attribute uh a double in in signups from LLMs.

>> I don't have a full answer. Uh but that is a good question. Uh but but I I I I kind of know but I I don't have the the full picture. >> Okay, no worries. Um >> but I know that their data science team, you know, spent time on it and that thoughtful about it. Yeah, pro probably they're doing a good job there. And this is why I was uh generally curious to

understand it in more detail, but um um I I could also totally see why um Web Flow and you would not be comfortable sharing it publicly, but um let's let's talk about uh another aspect of the whole attribution thing. Um you already mentioned that um most of the the searches or like the the the prompt entries are zero click. Um, can you see chat GBT and also then other LLMs or AI

chatbots go to showing more links over time or do you think like we are now locked in with the the the amount of links and the probably very low clickthrough rates we are at currently? No, I think it'll become way more clickable. And I think that LMS will start becoming more and more similar to search and we are already seeing that.

So if you ask something like where should I go in Hawaii or what's the best TV, you will see clickable shopping carts just like you see click clickable shopping carts in search and it's useful because I want to be able to I don't want to have to open a new tab and go find Ripling and click on their brand search ad on Google. Like that's not the ideal experience. Ideal experience is

just click on Ripling. So, uh, I think that it's very likely that things will become way more clickable. And how Google looked at this is in the early days, they had the 10 blue links and they added maps and shopping and uh uh travel and uh events and flight booking. So, I think the LMS will just go one by one and start having vertical specific uh experiences. And as a user, I want to

click on stuff. it's better if I can just click on something and and uh and go somewhere rather than having to open a new tab. I do also think that there will be autonomous Asians where maybe I don't need to click on something to actually buy or book. We're already starting to see that with uh you know I think Chach wants to do native native stuffs within Chachd. So you don't have

to leave to buy a product or to to book something. So I think that we might also see that. So it wouldn't be zero click but it would be click within LM. >> And how closely do you watch AI mode? because I mean now obviously you're in the US, we're in Europe. Um and AI mode is now also available but it has been rolled out um significantly with a significant delay also due to the EO

regulation obviously but um how important do you see Google's AI mode already being alongside CHBT or maybe also the more AI native interfaces like Perplexity. I think that when Google enters a new space, they have multiple teams working on the exact same thing with multiple solutions that all do something kind of similar and then they merge them. And uh

we've seen that multiple times. So I think what will happen is AI overviews, AI mode, and Gemini will eventually all merge because it's essentially doing the same thing. You don't need three different products that all do the same thing. So I think that they have multiple teams trying you know multiple really good teams trying this out and figuring things out and then they'll

merge them and there'll be a single unified experience. Now if you add AI overviews uh AI mode and Gemini that's a really big footprint and so I am generally looking at uh uh that but I I expect that it'll be a a converged unison of all three of these and then if you look at the data chat GBT and Gemini Gemini or sorry Google Gemini AI is not that far off from chat GBT and then

everyone else is a distant second. So those two OpenAI and uh and Google are by far the uh largest market share. And do you think like um OpenAI or Chhatti will be able to steal markets share like let's maybe think one two years in the future from Google or do you think Google will be able to either maintain the the status they still have or maybe even grow market share again because AI

because of this whole um um bringing together of AI mode AI and Gemini you just outlined. So there's the services and then there's the companies. So do I think that search will go down? I don't think that search will go down, but I think that search plus LLM usage will increase. So search will just stay flat and then LLM usage will be on top of that incremental.

Uh then the question is the two companies and will open AAI take market share from Google? I could definitely see that happening because Google has such a large market share for search. I think it's 95% depending on what you look look at. So it's hard to maintain 95% market share forever. I would expect that that eventually would move over to someone that could be open AAI that

could be, you know, a combination of others. That could be Bing, but I wouldn't expect that anyone in any category would ever have a 95% market share permanently. I would expect eventually something that high would would go down. I think Google had a little bit of an an awakening moment uh when uh I think it was Robbie Stein um joined the company

again and they um were basically already declared that uh because JGBT won market share and uh won um like what was it 700 800 million weekly active users and now Google somehow comes back with AI reviews and know also Gemini the new models suddenly being very popular also uh in in app download charts. So have you been surprised by this comeback or would you

even agree that this is it is a comeback by Google? It is a comeback by Google. Uh I'm not surprised. Uh but I'm I'm impressed. So why I say that is because there's the innovator's dilemma where big companies will eventually be disrupted by and this is a you know perfect example of a category that would lend itself to the innovator's dilemma where new start uh new entrant small

market big company cruise liner can't turn their boat quickly because they're not adaptive tiny boat comes in steals market share this exactly what the what the innovator's dilemma would lend itself to and that has not happened you know JBD has has blown up, but Google has has adapted and uh and is is growing quite well. And there are other companies who are not like Google's the

only one that's actually catching up to or even getting even close to catching up with uh OpenAI. I'll add that it's very hard. Open AAI was working on this for I don't even know 10 years plus. Like it's very hard to build uh an LLM uh and uh and and have the talent to do that. So the fact that Google has been able to do that is is quite impressive. So, I'm I'm not s I believe that this is

true. I am not surprised and I am very impressed. >> Great. Um let's go to some community community questions um again because we got some really good questions beforehand um from people that I definitely um yeah want to get your answer or at least your your ideas on. Um the first one we got was from um someone working at a review platform. So

for full for full context review platform in Germany so basically like Capter or G2 it's called OMR reviews uh like so like a software review platform and um the person asked what do platforms like review platforms need in terms of content and positioning so that users still actively come to their sites and not just consume the information indirectly via AI or LLM answers.

Well, what I would want would be, you know, I mentioned Reddit answers. So, Reddit answers is an LM summariz summarizing what happened on Reddit. What I would want would be a summary. I mentioned Sermo. Sermo is summarize what the doctors are saying on Sermo. I would want the review sites to have their own summarization. So, and and I don't want to read thousands of reviews. I I can't

I don't have time for that. What I want to know is generally what what are people saying. So, what I would really like would for a G2 or I think I forget the site that you mentioned is I would want an AI summarization of of the reviews. That would be super useful. And then maybe also the review the review flow can help guide me to add stuff to my answer to make it even more useful.

Like Airbnb does a good job about asking about the cleanliness and the responsiveness of the host. So, for a general review site about B2B products, have lots of reviews. encourage uh part of the reviews to answer the questions that people also have and some diversity of of opinion and then have your own AI summarization uh within your platform so I don't need to go somewhere

else. Okay, great answer. Um another question we got was do you think that what you shared on Lenny's podcast has aged well or would you change or add anything? >> I think it's aged pretty well. Yeah, I can't think of Well, I mean, would I add anything? I would I would add the the stuff that I have been sharing post, Lenny. I I mean, I have more more

insights today than I did in in uh September when I filmed it. I wouldn't take back anything. Uh I So, I asked after Lenny, I got a bunch of people adding me on LinkedIn and saying, "You had a great podcast." And then I asked follow-up questions to each of the people who said that the podcast was good. And I asked them, uh, I would love your feedback. Is there anything that

you disagree with? And what would you what did you find most useful? And is there anything that you would like me to to discuss more? And then I got about 50 answers. And then I did an AI summarization of the uh feedback that I got about the Lenny's podcast. Uh the number one request was help center optimization uh content. It's like tell me how to optimize my help center. And

uh I'm still early in that. I haven't mastered that yet. But I think what I would want to add would be I mean what I would want to add would be actual real case studies. So the more case studies the better which I didn't you know I shared what I had then and I have more now. And then the second would be some more specific tactical stuff around uh help center optimization and product

content content around describing how your product works and stuff like that. Can you share uh at least some of your early additional thoughts on help center optimization? Because I also saw help center content actually show up in citations um a couple of times now with clients we're working with when we talk about very specific also like integration related uh prompts and like

we're also trying to explore this but yeah obviously very selfishly I'd like to uh get your insights from it. >> Yes. So step one is what are people asking about? And we talked about how to do that, but I think customer support, sales calls, Reddit answers, what are people asking about my product? Uh G2, this this will give you information about what product

questions people have. And then step two is answering the question. I don't think that there's anything nonobvious about this. If somebody says, "What meeting note transcription uh tool integrates with Zoom?" have a page about that and say that you integrate with Zoom and explain how you do that. That's really all you need to do. Otter shows up for that particular example. They

show up because they have a page saying we have a integration with Zoom. Here's how it works. The the non-obvious thing with that particular example is have multiple pages describing this. So if you ask that question meeting transcription uh product integrates with Zoom, there's a help center page on Otter. There's a feature page on like Otter/Zoom or something like that.

There's an article about it like how to use it. Then there's uh landing pages on Zoom about the Otter integration. So there's all these different pages and so have have multiple takes at answering this particular question and especially if uh have a page on the integration partner site. So, let's say that I'm a brand new meeting note uh transcription tool and I integrate with Zoom and no

one's heard of me. Ping the people, you know, assuming you're friends with with Zoom, ask them to make a page about your integration. Like all your integration partners, have them have a page about your amazing integration and then you'll show up twice. That's a good one. That makes a lot of sense. Um, we have another question. We

maybe covered a little bit of that, but I still want to um be sure to ask it. It's where and why do you see most AI content workflows fail? It's because you're just prompting chat GPT to derive information from the public and nothing more. It's just a wrapper around chat GPT. I think that a better version of that is you have something like you know the apartments

in in city example people want to know about the price and the features and the areas to get an apartment and the pros and cons. Okay, now I have unique metadata for each of those four different questions. Then I have a workflow and a prompt saying summarize the cost, you know, the information about the price and include this, this, and this. That's what they're lacking.

It's like somebody being thoughtful about configuring the prompt and feeding in unique information to get an output that is more useful than just a derivative of what's already available to everyone. >> Got it. Um, this is a community question for myself. Um, do you think more people should get a super me? Yes. Well, if you have more people

should use SuperM for sure and well I I'll explain what Superme is. So Superme is my friend's company where you can feed in your thought leadership and LinkedIn content and stuff like that and then you can have a super Ethan where it takes all of my webinars and my articles and my LinkedIn posts and things like that and then you can ask AI Ethan what you know

what tools do you like? how would you approach building on an AEO strategy? And the answer is a summarization of my thought leadership. So, it's not a derivative of public information. It's Ethan's specific thought leadership. So, it's quite useful. It's only as useful as the input. So, if you have a bunch of novel ideas that you've documented, then you should have a superme. If you

haven't done that, no judgment. Probably no reason for you to have a superme. So, that's how I think about it. But, but every everyone should be using it. I I I um I looked at I have had over 500 questions to super Ethan and uh the answers are pretty good. >> Yeah. Um it it looked very interesting. Uh probably uh you have to do interesting stuff and have interesting

things to say so that you qualify to having a superme. Um I actually have two more questions. Um one is one actually came from your uh colleague Emily. Um, so don't blame me, please. Um, she wanted to know what's your favorite Backstreet Boys song. >> Okay, the answer to this is my favorite I I have to say In Sync is my favorite uh over Backstreet Boys and my favorite

song is Tearing Up My Heart. And my [clears throat] favorite performance is the I think it's the 1999 MTV VMAs with Britney Spears and in Sync uh Tearing Up My Heart. Uh that's one of the best performances of all time. Okay. So then but the the answer to Backstreet Boys is I would say uh Shape of My Heart is my favorite Backstreet Boys song. I very much want to see them at the sphere in

Las Vegas. So I hope uh that they continue their residency so that I can see them live. >> Okay. And in this case, you wouldn't risk asking Reddit answers if it's good and because if it says it's not good, you don't want to be spoiled because you definitely want to see it. I've heard good things. I think I checked Reddit answers and I heard good

things. My my my one of my biggest regrets is I didn't get to see Britney at uh at her residency in uh Las Vegas and I've never seen her perform. So, I can't make that mistake again and I have to see the Backstreet Boys. >> Okay. Thanks. Then the last one um and I think actually just um bluntly stole this from Lenny, but I also started asking it on my podcast and the um

answers were so great that I just um wanted to also have it in here. Um what's something that we didn't talk about but should have talked about? where to get information on like who to read, where to get information uh about the subject is what I would say. >> Yeah. Because there's not there's not that much information. Uh I mean there's ideas, there's there's conversation, but

there's not there's a lack of first party experiment data, you know. Uh so what would you recommend people to go to or where do you yourself like get your information from? I like uh Kevin Indig does good stuff. Lily Ray does good stuff. Uh both AHFs and SCM Rush and Surfer all do pretty good uh thought leadership and studies. And

there's probably others. I don't I I I have a full-time job, so I'm not spending all day long reading other people's stuff. So there's probably many people that I did not that I have not read uh that that are doing interesting things, but those are people that I personally read that I find useful. >> Awesome. Ethan, this has been a very insightful

conversation. I feel like it's a very decent follow-up to the Lenny's episode. Thanks so much for sharing um so much also such indepth um thoughts, maybe also some raw thoughts. Um, I really appreciate it. Um, I hope also that everybody listening and um, viewing had a great time. I think it's obvious if you want to learn more about what Ethan is doing, either follow him on LinkedIn

or go to Superme. I think it's uh, superme.ai/eith. So like Ethan, but just with the E Smith. um and ask questions to Superme Ethan before you ask Ethan because he has a full-time job. So, please respect it people. But um yeah, Ethan, thanks so much for coming on. Um was was um a real pleasure. >> Thank you for having me.
